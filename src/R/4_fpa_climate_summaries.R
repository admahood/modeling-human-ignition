
if (!exists("fpa_ll")) {
  if (file.exists(file.path(processed_dir, "fpa_ll.gpkg"))) {

    fpa_ll <- st_read(file.path(processed_dir, "fpa_ll.gpkg")) %>%
      mutate(year_month_day = floor_date(ymd(DISCOVERY_DATE), "month"))

  } else if (!file.exists(file.path(processed_dir, "fpa_clean.gpkg"))) {

    fpa_ll <- fpa_clean %>%
      st_transform(crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0") %>%
      mutate(year_month_day = floor_date(ymd(DISCOVERY_DATE), "month"))

    st_write(fpa_ll, file.path(processed_dir, "fpa_ll.gpkg"), overwrite = TRUE)

    system(paste0("aws s3 sync ",
                  processed_dir, " ",
                  s3_proc_prefix))

  } else if (exists("fpa_clean")) {

    fpa_ll <- st_read(file.path(processed_dir, "fpa_clean.gpkg")) %>%
      st_transform(crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0") %>%
      mutate(year_month_day = floor_date(ymd(DISCOVERY_DATE), "month"))

    st_write(fpa_ll, file.path(processed_dir, "fpa_ll.gpkg"))

    system(paste0("aws s3 sync ",
                  processed_dir, " ",
                  s3_proc_prefix))
  }
}

stat <- c('mean', 'numdays95th', '95th')
vars <- c('aet', 'def', 'ffwi', 'fm100', 'pdsi', 'pr', 'tmmx', 'vpd', 'vs')

unique_states <- unique(fpa_ll$STATE)

for (j in stat){
  for (i in vars) {
    fpa_summaries <- list()
    counter <- 1

    tifs <- check_tifs(j, i)

    if (is.null(tifs)) {

      print(paste0('Skipping ', j, ' ', i))

    } else if (!file.exists(file.path(summaries_dir, j, paste0('fpa_', i, '_', j, '_summaries.rds')))) {

      # subset the fpa-fod data based on state grouping variable
      # this increases the speed of this function and only needs ~40GB of memory max.
      for (k in 1:length(unique_states)) {

        # create a subdataframe based on state subset
        sub_df <- subset(fpa_ll, fpa_ll$STATE == unique_states[k])

        print(paste0('Working on ', counter, " of ", length(unique_states), " for the ", j, ' ', i))

        # setup parallel environment
        cl <- makeCluster(detectCores())
        registerDoParallel(cl)

        # extract climate time series data based on point or polygon locations.
        # this extract is pulling in point data, so fun = mean does not matter
        extractions <- foreach (i = tifs) %dopar% {

          res <- raster::extract(raster::stack(i), sub_df,
                                 na.rm = TRUE, fun = 'mean', df = TRUE)
        }
        stopCluster(cl)

        # ensure that they all have the same length
        stopifnot(all(lapply(extractions, nrow) == nrow(sub_df)))

        # convert to a data frame
        extraction_df <- extractions %>%
          bind_cols %>%
          as_tibble %>%
          mutate(index = ID) %>%
          select(-starts_with("ID")) %>%
          rename(ID = index) %>%
          mutate(FPA_ID = data.frame(sub_df)$FPA_ID) %>%
          dplyr::select(-starts_with('X')) %>%
          gather(variable, value, -FPA_ID, -ID) %>%
          filter(!is.na(value)) %>%
          mutate(FPA_ID = as.factor(FPA_ID),
                 ID = as.factor(ID)) %>%
          # clean the final, long climate data frame with linked fpa ids
          separate(variable,
                   into = c("variable", 'year', "statistic", "month"),
                   sep = "_|\\.") %>%
          mutate(day = '01',
                 year_month_day = as.Date(paste(year, month, day, sep='-')))

        # reduce the size of the dataframe to be joined during the get_climate_lags
        sub_df <- sub_df %>%
          dplyr::select(FPA_ID, year_month_day)

        # run get_climate_lags for the prior 24 months given a fpa-fod fire event
        # this will iteratively populate a list given each state grouping variable
        fpa_summaries[[k]] <- get_climate_lags(sub_df, extraction_df, sub_df$year_month_day, time_lag = 24) %>%
          dplyr::select(-year_month_day, - ymd_lagged)

        counter <- counter + 1
      }

      if (length(fpa_summaries) > 0) {
        # rbinds the iteratively populated list and creates a cohesive dataframe
        fpa_summaries <- do.call(rbind, fpa_summaries) # Convert to data frame format

        # save the final cleaned climate extractions joined with the fpa-fod database
        summary_name <- file.path(summaries_dir, j, paste0('fpa_', i, '_', j, '_summaries.rds'))
        write_rds(fpa_summaries, summary_name)

        # push to S3
        system('aws s3 sync data/summary s3://earthlab-modeling-human-ignitions/summary')
      }
    }

    # creating the final dataframe takes about 150 GB of RAM, this clears the cache so it can be run again
    gc()
  }
}
